import comp from "E:/Myown/Blog/.temp/pages/dl/llm/LLM.html.vue"
const data = JSON.parse("{\"path\":\"/dl/llm/LLM.html\",\"title\":\"LLM综述\",\"lang\":\"zh-CN\",\"frontmatter\":{\"title\":\"LLM综述\",\"author\":\"xbZhong\",\"isOriginal\":true,\"article\":true,\"category\":\"llm\",\"timeline\":true,\"icon\":\"carbon:model\",\"date\":\"2025-08-20T00:00:00.000Z\",\"description\":\"本页PDF 大模型训练过程 预训练 目标：学会语言本身，语言的通用表示，进行语言建模 方法：自监督学习 SFT（有监督微调）：何使用标注数据对预训练模型进行监督训练的过程 目标：让模型遵循指令，能够理解人类提出的问题 方法：监督学习 通常有全参微调，LoRA微调，Adapter微调等，指令微调也是SFT的一部分 RLHF（人类反馈强化学习） 目标：让模...\",\"head\":[[\"script\",{\"type\":\"application/ld+json\"},\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Article\\\",\\\"headline\\\":\\\"LLM综述\\\",\\\"image\\\":[\\\"https://xbzhong.cn/screenshot/dl/image-20250817002125325.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250817115944207.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250817120703506.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250817175244201.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250817175237288.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250817181356989.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250817181417147.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250814165126760.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250813101824156.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250814170149855.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250814170444276.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250814171201197.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250814172727460.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250815105857273.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250815105921459.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250815223849142.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250815225054421.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250815224552849.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250815235148856.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250816005408042.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250816005535158.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250813120951515.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250813121114207.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250813115950009.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250813115500851.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250813115927153.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250816120438735.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250816120905940.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250816131927886.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250816133715163.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250816135113159.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250819121745375.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250819121817645.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250819121841203.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250820134630626.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250819141437101.png\\\",\\\"https://xbzhong.cn/screenshot/dl/image-20250819132629103.png\\\"],\\\"datePublished\\\":\\\"2025-08-20T00:00:00.000Z\\\",\\\"dateModified\\\":null,\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"xbZhong\\\"}]}\"],[\"meta\",{\"property\":\"og:url\",\"content\":\"https://xbzhong.cn/dl/llm/LLM.html\"}],[\"meta\",{\"property\":\"og:site_name\",\"content\":\"阿b的博客\"}],[\"meta\",{\"property\":\"og:title\",\"content\":\"LLM综述\"}],[\"meta\",{\"property\":\"og:description\",\"content\":\"本页PDF 大模型训练过程 预训练 目标：学会语言本身，语言的通用表示，进行语言建模 方法：自监督学习 SFT（有监督微调）：何使用标注数据对预训练模型进行监督训练的过程 目标：让模型遵循指令，能够理解人类提出的问题 方法：监督学习 通常有全参微调，LoRA微调，Adapter微调等，指令微调也是SFT的一部分 RLHF（人类反馈强化学习） 目标：让模...\"}],[\"meta\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"meta\",{\"property\":\"og:image\",\"content\":\"https://xbzhong.cn/screenshot/dl/image-20250817002125325.png\"}],[\"meta\",{\"property\":\"og:locale\",\"content\":\"zh-CN\"}],[\"meta\",{\"property\":\"article:author\",\"content\":\"xbZhong\"}],[\"meta\",{\"property\":\"article:published_time\",\"content\":\"2025-08-20T00:00:00.000Z\"}]]},\"readingTime\":{\"minutes\":36.18,\"words\":10853},\"filePathRelative\":\"dl/llm/LLM.md\",\"excerpt\":\"<p><a href=\\\"/pdfs/dl/llm/LLM.pdf\\\">本页PDF</a></p>\\n<h2>大模型训练过程</h2>\\n<ol>\\n<li><strong>预训练</strong>\\n<ul>\\n<li>目标：学会<strong>语言本身</strong>，<strong>语言的通用表示</strong>，进行语言建模</li>\\n<li>方法：自监督学习</li>\\n</ul>\\n</li>\\n<li><strong>SFT（有监督微调）</strong>：何使用标注数据对预训练模型进行监督训练的过程\\n<ul>\\n<li>目标：让模型遵循指令，能够<strong>理解</strong>人类提出的问题</li>\\n<li>方法：监督学习</li>\\n<li>通常有全参微调，LoRA微调，Adapter微调等，<strong>指令微调</strong>也是SFT的一部分</li>\\n</ul>\\n</li>\\n<li><strong>RLHF（人类反馈强化学习）</strong>\\n<ul>\\n<li>目标：让模型更符合人类的<strong>偏好</strong>，贴近人类<strong>意图</strong></li>\\n<li>方法：强化学习</li>\\n</ul>\\n</li>\\n<li><strong>增强与扩展</strong>\\n<ul>\\n<li>蒸馏，微调，迁移学习等</li>\\n</ul>\\n</li>\\n</ol>\",\"autoDesc\":true}")
export { comp, data }

if (import.meta.webpackHot) {
  import.meta.webpackHot.accept()
  if (__VUE_HMR_RUNTIME__.updatePageData) {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  }
}

if (import.meta.hot) {
  import.meta.hot.accept(({ data }) => {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  })
}
