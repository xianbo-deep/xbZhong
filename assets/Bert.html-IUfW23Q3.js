import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,e as t,o as e}from"./app-CwI-y_DA.js";const l={};function p(m,s){return e(),n("div",null,[...s[0]||(s[0]=[t('<h2 id="bert" tabindex="-1"><a class="header-anchor" href="#bert"><span>Bert</span></a></h2><p><strong>架构</strong></p><p>基于EMLO与GPT改进，融合了这两个模型的特征：</p><ul><li>EMLO使用的是<strong>双向的RNN</strong></li><li>GPT使用的是<strong>transformer</strong></li></ul><p>而Bert使用的是<strong>双向transformer</strong></p><p><strong>两步工作</strong></p><ol><li>无监督学习的预训练 <ul><li>使用大量无标记的文本进行预训练，也就是文字填空和NSP，使得模型学习到词元的特征和句子之间的关系</li></ul></li><li>有监督学习的微调 <ul><li>使用有标记的文本进行微调，目的是为了让模型适应下游任务，<strong>原始论文采用全参微调</strong></li><li>微调会保留Bert的预训练权重，但在任务数据上继续训练，并加上<strong>适应该任务的头部层</strong></li></ul></li></ol><p><strong>无标号的大数据</strong>的训练比<strong>有标号的一定大小的数据</strong>的训练有用</p><p><strong>解决的任务</strong></p><ol><li>做文字填空（MLM） <ul><li>会随机mask掉一些词，然后根据上下文进行词的填空</li><li>有<strong>15%<strong>的概率选中一些词，又有</strong>80%<strong>的概率把</strong>这些词mask掉</strong>，还有10%的概率换成词表原有的词，剩下10%的概率什么都不做</li><li>这种做法会导致bert在进行微调的时候<strong>有性能损失，因为微调的时候并不会去进行掩码</strong></li></ul></li><li>做下一个句子的预测**（NSP）** <ul><li>随机抽取两个句子，判断其中一个句子是不是另一个句子的下一句</li><li>有<strong>50%的正样本，50%的负样本</strong></li></ul></li></ol><p><strong>输入向量</strong></p><p>输入 = <strong>Token Embedding</strong> + <strong>Segment Embedding</strong> + <strong>Position Embedding</strong></p><ul><li><strong>Token Embedding</strong>：WordPiece分词（处理未登录词如&quot;unhappiness&quot;→&quot;un&quot;, &quot;##happiness&quot;）。</li><li><strong>Segment Embedding</strong>：标记句子归属（第一句为0，第二句为1）。</li><li><strong>Position Embedding</strong>：绝对位置编码（最长支持512个Token）。</li></ul><p><strong>文本标记</strong></p><ul><li><code>[CLS]</code>：这是一个向量，它会与句中所有其他词交互，聚合全局语义信息，它的<strong>最终隐藏层状态会作为分类器的输入特征</strong>，可以用于分类任务</li><li><code>[SEP]</code>：这个标记用于明确句子边界</li></ul><h3 id="wordpiece" tabindex="-1"><a class="header-anchor" href="#wordpiece"><span>WordPiece</span></a></h3><p>基于子词的一种分词算法</p><p><strong>核心目标</strong>：把单词拆分为<strong>更小的、有语义的子词单元</strong></p><ul><li><strong>减小词表大小</strong>：避免为所有单词单独分配向量</li><li><strong>提升泛化能力</strong>：通过子词组合处理未见过的词</li></ul><p>合并策略是基于<strong>语言模型的似然概率</strong>，<strong>优化目标是最大化语料库的似然概率</strong></p><p><strong>前缀标记</strong>：非词首子词用<code>##</code>标记（如<code>##ing</code>），区分词内和词首位置</p><h4 id="训练阶段" tabindex="-1"><a class="header-anchor" href="#训练阶段"><span>训练阶段</span></a></h4><ol><li>将语料库中所有单词拆分为<strong>单字符，统计字符概率</strong></li><li>计算候选子词对的得分，使用<strong>似然增益公式</strong>，将<strong>得分最高的子词对加入词汇表</strong>，直到词汇表达到预设大小</li></ol><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>−</mo><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">score(s) = \\log P(s)- (\\log P(s_1) + \\log P(s_2)) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">score</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span></p><p>​ 其中：</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span>：合并后的子词s在语言模型中的概率</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(s_1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(s_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>：未合并时<strong>两个子词独立出现的概率</strong></li></ul>',26)])])}const r=a(l,[["render",p]]),c=JSON.parse('{"path":"/dl/llm/Bert.html","title":"Bert","lang":"zh-CN","frontmatter":{"title":"Bert","description":"LLM开山鼻祖","author":"xbZhong","isOriginal":true,"article":true,"category":"llm","timeline":true,"icon":"carbon:model","date":"2025-07-29T00:00:00.000Z","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Bert\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-07-29T00:00:00.000Z\\",\\"dateModified\\":\\"2025-10-10T12:45:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"xbZhong\\"}]}"],["meta",{"property":"og:url","content":"https://xbzhong.cn/dl/llm/Bert.html"}],["meta",{"property":"og:site_name","content":"阿b的博客"}],["meta",{"property":"og:title","content":"Bert"}],["meta",{"property":"og:description","content":"LLM开山鼻祖"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-10-10T12:45:16.000Z"}],["meta",{"property":"article:author","content":"xbZhong"}],["meta",{"property":"article:published_time","content":"2025-07-29T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-10-10T12:45:16.000Z"}]]},"git":{"createdTime":1760100316000,"updatedTime":1760100316000,"contributors":[{"name":"BO","username":"BO","email":"2396768163@qq.com","commits":1,"url":"https://github.com/BO"}]},"readingTime":{"minutes":2.6,"words":780},"filePathRelative":"dl/llm/Bert.md","excerpt":"<h2>Bert</h2>\\n<p><strong>架构</strong></p>\\n<p>基于EMLO与GPT改进，融合了这两个模型的特征：</p>\\n<ul>\\n<li>EMLO使用的是<strong>双向的RNN</strong></li>\\n<li>GPT使用的是<strong>transformer</strong></li>\\n</ul>\\n<p>而Bert使用的是<strong>双向transformer</strong></p>\\n<p><strong>两步工作</strong></p>\\n<ol>\\n<li>无监督学习的预训练\\n<ul>\\n<li>使用大量无标记的文本进行预训练，也就是文字填空和NSP，使得模型学习到词元的特征和句子之间的关系</li>\\n</ul>\\n</li>\\n<li>有监督学习的微调\\n<ul>\\n<li>使用有标记的文本进行微调，目的是为了让模型适应下游任务，<strong>原始论文采用全参微调</strong></li>\\n<li>微调会保留Bert的预训练权重，但在任务数据上继续训练，并加上<strong>适应该任务的头部层</strong></li>\\n</ul>\\n</li>\\n</ol>"}');export{r as comp,c as data};
