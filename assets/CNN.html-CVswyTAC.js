import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,e as t,o as e}from"./app-sdAdeonL.js";const l="/screenshot/dl/image-20250417180348074.png",i={};function r(o,a){return e(),n("div",null,[...a[0]||(a[0]=[t('<h2 id="convolutional-neutral-network-卷积神经网络" tabindex="-1"><a class="header-anchor" href="#convolutional-neutral-network-卷积神经网络"><span>Convolutional Neutral Network（卷积神经网络）</span></a></h2><p><strong>局部连接</strong>：</p><p>CNN 不是每个神经元都连全图（像 MLP 那样），而是只看<strong>小块区域</strong>（比如 3×3）</p><p><strong>权重共享</strong>：</p><p>整个图上，这个卷积核在每个区域滑动时 <strong>用的都是同一套参数</strong></p><ul><li>在<strong>CNN</strong>里面，通常输入的张量维度会很大，在进行识别类别的时候，只需要让每个<strong>Neutral去负责自己的Receptive field</strong>，从而识别特征，<strong>不需要</strong>看整张图片</li><li><strong>卷积神经网络(CNN)训练的核心就是学习滤波器(filter)中的参数数值</strong></li></ul><h3 id="fully-connected-layer-全连接层" tabindex="-1"><a class="header-anchor" href="#fully-connected-layer-全连接层"><span>Fully Connected Layer（全连接层）</span></a></h3><ul><li>层中的每个神经元都与前一层的所有神经元相连接</li><li>每个神经元有独立的权重参数</li><li>通常用于网络末端进行分类或回归</li><li>参数多，更容易过拟合</li></ul><h3 id="receptive-field-感受野" tabindex="-1"><a class="header-anchor" href="#receptive-field-感受野"><span>Receptive field（感受野）</span></a></h3><ul><li><p>CNN中的每个神经元只连接到输入图像的<strong>一个局部区域</strong>（而不是整个输入），这个局部区域就是该神经元的感受野</p></li><li><p>感受野大小可以不一，可以只考虑某些<strong>channel</strong></p></li><li><p>彼此之间可以<strong>重叠</strong>，可以学习到图像中的<strong>所有特征</strong></p></li><li><p>一个感受野可以由<strong>多个</strong>神经元学习</p></li></ul><h4 id="typical-setting" tabindex="-1"><a class="header-anchor" href="#typical-setting"><span>Typical Setting</span></a></h4><p><em>Description of Receptive field</em>：</p><ul><li><strong>kernel size</strong>（卷积核）：用于执行卷积操作的滤波器（或称为卷积核） <ul><li>常见大小：3x3，更大的卷积核可以学习到更多的特征，获得更大的感受野，但也会提高计算成本</li></ul></li><li><strong>stride</strong>（步长）：步长 ，包括横向步长和纵向步长 <ul><li>步长一般设置为1或2，需要注意的是图像可以重叠学习，即当步长小于卷积核尺寸时，卷积操作会在输入上产生重叠，这有助于学习更丰富的特征表示</li></ul></li><li>**padding（**补丁）：当感受野（卷积核）超出图像范围，需要对超出范围进行参数补充，称为padding</li><li><strong>输出尺寸计算</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>输出尺寸</mtext><mo>=</mo><mo stretchy="false">(</mo><mtext>输入尺寸</mtext><mo>−</mo><mtext>卷积核尺寸</mtext><mo>+</mo><mn>2</mn><mo>×</mo><mi>p</mi><mi>a</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">输出尺寸 = (输入尺寸 - 卷积核尺寸 + 2 × padding) / stride + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">输出尺寸</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord cjk_fallback">输入尺寸</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord cjk_fallback">卷积核尺寸</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">a</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></li></ul><h3 id="parameter-sharing" tabindex="-1"><a class="header-anchor" href="#parameter-sharing"><span>Parameter Sharing</span></a></h3><p><strong>负责不同感受野的不同神经元的参数完全一致</strong></p><ul><li>共享参数，使得计算成本低，防止<strong>OverFitting</strong></li></ul><h4 id="typical-setting-1" tabindex="-1"><a class="header-anchor" href="#typical-setting-1"><span>Typical Setting</span></a></h4><ul><li>一个感受野会由多个神经元负责学习</li><li>不同感受野的同组神经元<strong>参数一致</strong></li></ul><h3 id="tensor-张量" tabindex="-1"><a class="header-anchor" href="#tensor-张量"><span>Tensor（张量）</span></a></h3><ul><li>张量是多维矩阵</li><li>一张图片可以由三维张量表示 <ul><li>两个维度是图片长和宽</li><li>还有一个维度是图片的<strong>Channels（通道）</strong>，可以理解为RGB通道，即三原色</li></ul></li></ul><h3 id="convolutional-layer-卷积层" tabindex="-1"><a class="header-anchor" href="#convolutional-layer-卷积层"><span>Convolutional Layer（卷积层）</span></a></h3><ul><li>一层卷积层有多个<strong>Filter</strong>（卷积核） <ul><li>不同的卷积核会在同一个感受野上进行学习</li><li>不同的卷积核学习不同的特征</li><li>最后这层卷积层的输出是一个<strong>Feature Map</strong>，是三维张量 <ul><li><strong>Feature Map</strong>是三维张量是因为不同的<strong>Filter</strong>学习了不同的特征，一个特征对应一个通道，即通道数直接等于Filter的数量（每个Filter贡献一个通道）</li></ul></li></ul></li></ul><h3 id="pooling-池化-——maxppooling" tabindex="-1"><a class="header-anchor" href="#pooling-池化-——maxppooling"><span>Pooling（池化）——MaxpPooling</span></a></h3><p><strong>没有要学习的参数！！！</strong></p><ul><li>池化一般在卷积之后进行，是为了让图片变得更小，易于处理，减少运算量</li><li>一般是几次<strong>Convolution</strong>之后做一次Pooling</li></ul><h3 id="flatten-展平" tabindex="-1"><a class="header-anchor" href="#flatten-展平"><span>Flatten（展平）</span></a></h3><ul><li><p>无需要学习的参数</p></li><li><p>将<strong>多维输入数据转换为一维向量</strong></p></li><li><p>常用于连接**卷积层（CNN）和全连接层（FC）**之间的过渡</p></li></ul><h3 id="the-whole-cnn" tabindex="-1"><a class="header-anchor" href="#the-whole-cnn"><span>The Whole CNN</span></a></h3><figure><img src="'+l+'" alt="image-20250417180348074" tabindex="0" loading="lazy"><figcaption>image-20250417180348074</figcaption></figure>',29)])])}const c=s(i,[["render",r]]),g=JSON.parse('{"path":"/dl/cv/CNN.html","title":"卷积神经网络","lang":"zh-CN","frontmatter":{"title":"卷积神经网络","description":"讲的比较抽象，最好结合视频理解","author":"xbZhong","isOriginal":true,"article":true,"category":"cv","timeline":true,"icon":"lucide:network","date":"2024-06-01T00:00:00.000Z","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"卷积神经网络\\",\\"image\\":[\\"https://xbzhong.cn/screenshot/dl/image-20250417180348074.png\\"],\\"datePublished\\":\\"2024-06-01T00:00:00.000Z\\",\\"dateModified\\":\\"2025-10-10T12:45:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"xbZhong\\"}]}"],["meta",{"property":"og:url","content":"https://xbzhong.cn/dl/cv/CNN.html"}],["meta",{"property":"og:site_name","content":"阿b的博客"}],["meta",{"property":"og:title","content":"卷积神经网络"}],["meta",{"property":"og:description","content":"讲的比较抽象，最好结合视频理解"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://xbzhong.cn/screenshot/dl/image-20250417180348074.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-10-10T12:45:16.000Z"}],["meta",{"property":"article:author","content":"xbZhong"}],["meta",{"property":"article:published_time","content":"2024-06-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-10-10T12:45:16.000Z"}]]},"git":{"createdTime":1760100316000,"updatedTime":1760100316000,"contributors":[{"name":"BO","username":"BO","email":"2396768163@qq.com","commits":1,"url":"https://github.com/BO"}]},"readingTime":{"minutes":3.06,"words":918},"filePathRelative":"dl/cv/CNN.md","excerpt":"<h2>Convolutional Neutral Network（卷积神经网络）</h2>\\n<p><strong>局部连接</strong>：</p>\\n<p>CNN 不是每个神经元都连全图（像 MLP 那样），而是只看<strong>小块区域</strong>（比如 3×3）</p>\\n<p><strong>权重共享</strong>：</p>\\n<p>整个图上，这个卷积核在每个区域滑动时 <strong>用的都是同一套参数</strong></p>\\n<ul>\\n<li>在<strong>CNN</strong>里面，通常输入的张量维度会很大，在进行识别类别的时候，只需要让每个<strong>Neutral去负责自己的Receptive field</strong>，从而识别特征，<strong>不需要</strong>看整张图片</li>\\n<li><strong>卷积神经网络(CNN)训练的核心就是学习滤波器(filter)中的参数数值</strong></li>\\n</ul>"}');export{c as comp,g as data};
